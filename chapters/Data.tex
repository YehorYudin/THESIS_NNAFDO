% !TEX root = ../main.tex

\section{Data Preparation}

In this section we describe what are the data used to train and evaluate the inference system.
We discus the parameters used to define the Topology Optimization problem and their variability and properties.
The other aspect to be covered is the ways to represent these data in a way suitable for ANN system input.
Lastly, we discuss the aspects of choosing the proper training dataset for the inference system.

\subsection{Topology Optimization Problem Cases}

The physical problem for which the Topology Optimization (TO) is performed is defined by the Boundary Conditions (BC) and the force applied to the element. 
The former has a view of Dirichlet BC and define the parts of the element that experience no displacement and the latter is in general form a filed of forces applied to all the point of the element. 
Translated in the discretized form used in the Finite Element Analysis (FEA) performed during the TO, the former defined the fixed degrees of freedom and the latter defines the non-zero elements of the right-hand-side vector of loads.
\medskip

To obtain a set of TO results that could be used as training and testing samples for the Artificial Neural Network (ANN) we choose to perform TO for various combination of values of the boundary conditions and load cases.
\medskip

\subsubsection{2D Cases of Topology Optimization}

As Boundary Condition we can choose to fix one of sides of the domain (one of four walls). 
We can also choose to leave the inner $\frac{1}{3}$ of the side unfixed or fix only the extreme points of the wall. 
\todo{(Add picture of boundary conditions and add it as a caption)}
\medskip

As a simple load case we consider only the situation where a force is applied to a single point of the element. 
The variable parameters are the coordinates of the force application (X, Y) and the direction of the force.
We keep the magnitude of the force in every direction constant and choose one of the eighth possible directions: along one of the axis in one of the direction or a combination of those.
The last parameter that defines the result of TO is the value of volume fraction occupied by the material, for which we choose $3$ values : $0.1, 0.2, 0.4$
\medskip

In the end we obtain number of possible input sample equal to $4\,\mathrm{(fixed \, side)} \; \times \; 3\,\mathrm{(type \, of \, BC)} \\ \; \times \; 3\,\mathrm{(value \; of \; Volume \; fraction)} \; \times \; 8\,\mathrm{(force \, direction)} \; \times \; 32\,\mathrm{(Y \, coordinate)} \; \times \; 16\,\mathrm{(X \, coordinate)} = 147446$ bitmaps.

\subsubsection{3D Case of Topology Optimization}

For the second series of experiments we required 3D models of optimized elements.
Because of the larger design space, and larger problem size, which lead to much larger time spent to obtain a single result, we decided to restrict data variability.
For the 3D case we left only a single volume fraction value equal to $0.2$, as well as single Boundary Condition case when there are four corner elements of the $ X=0 $ edge of domain are fixed.
The problem domain we considered has size of $32 \times 32 \times 32$ hexahidral elements, and we performed TO for cases when a force was applied for a single element. 
We generated TO results not for every  possible point of load application, but took point with step of 4 in every dimension in the furthermost half of the domain in $Y$ direction. 
For every point of load application we considered forces of magnitude $1$ directed to each face of the hexagedral element, as well as towards each vertex, which corresponds to $D3Q15$ model of hexahidral directions. \todo{maybe unsuitable}
In total, for the entire dataset we generated $ 14\,\mathrm{(force \, direction)} \; \times \; 5\,\mathrm{(Y \, coordinate)} \; \times \; 9\,\mathrm{(X \, coordinate)} \times \; 9\,\mathrm{(Y \, coordinate)} = 5670 $ models.


\subsection{Dataset generation}

For the purpose of generating dataset of known solved TO problems, in this work we used the MATLAB code provided in the paper "A 99 line topology optimization code written in Matlab"\cite{to_99line}.
It minimizes compliance for 2D material layout under some load.
The code uses square elements for finite element discretization and uses power-law model to represent the compliance as the goal function, as described in the introduction section.
\medskip

The code was modified in order to have several arbitrary points where the load is applied.
A solution of single TO problem is called by calling the function  , where all are passed as parameters, such problem dimension, volume fraction, power penalty, convergence criterion, coordinated of force application points and force vector. 
The script saved the resulting material layout as a gray-scale PNG image where every pixel value denotes the virtual material density of corresponding square finite element.
For the typical problem size of $ 32 \times 32 $ single TO problem solution took avaragly $1 \mathrm{s}$ using a single CPU.
Totally there were $~150$ thousands  created
\todo{fill in reference, number, code etc.} 
\medskip

To generate results of compliance minimization for 3D models we used a program IDeAs\cite{.}, created a Siemens CT.
The program the parameters of the TO problem are defined in a JSON file, in which one should  state STP files describing geometrical domain, fixed regions for boundary conditions, and surfaces at which force is applied.
The boundary condition type, force vector and volume fraction value are also stated, as well as parameters of physics, discretization and analyzes. 

Run with $32$ threads on a single Nvidia GeForce 1080 GTX GPU we generated $~5$ thousands of $32 \times 32 \times 32$ voxels model of result models, witch took $10$ seconds.
 

% The Boundary Coditions used during Topology Optimization as an input to the Finite Element Analysis constitute of the fixed degrees of freedom


\subsection{Inputs of the ANN Model}

In our case for every fixed domain we use every TO problem is defined by by boundary conditions, filed of applied forces and volume fraction occupied by material. 
These are three parameters that vary from case to case of TO problem and others stay the same, thus the model should be able to infer proper layout using this varying information.
One of issues during the design of the model was how the model should account this information.
\medskip

Boundary condition is defined by a geometrical locus on the domain for which values of some function is predefined, in our case it means the displacement vector for these points is equal to zero. 
After the discretization it is expressed in having some degrees of freedom fixed. 
%Essentially that means that for some of the geometrical elements the component in the displacement vector should be always put to zero which is reflected in the from of the right hand side 
Since our domain has a correspondence with a bitmap where every element of the TO problem maps to an element of an array we represent boundary conditions with an array where elements with indices equal to coordinates of fixed element has vale of $1$ and $0$ for everything else.
\medskip

In similar manner we represent the force field applied to the element.
%Generally we have only a few points at which forces are applied, which after discretization will be reflected in a few elements for which there will be values filled in teh right hand side.
We have two different bitmaps for $X$ and $Y$ components of the force or every point at which force is applied we fill the array element with the $+1$ or $-1$ value depending on the force direction, leaving everything else $0$.
\medskip

The Volume Fraction is essentially a scalar value and it is accounted in the system by introducing a new channel tiled with a single value at the layer of the representation in the lowest dimensionality as described in the ``Architecture'' section.
\medskip

The bitmap of boundaries, force in $X$ direction and force in $Y$ direction form the three input channels of the model over which the convolutions are applied.
\medskip

The one problem with such representation of BC and force fields that there are only a few non-zero elements in the channel that leads very little information influx in the model and makes it virtually impossible to train the model.
\todo{explain more, demonstrate?}
\medskip

As a remedy for this problem we populate the input channels with more non-zero elements representing meaningful information by finding a Signed Distance Field (SDF) of every channel.
\medskip

For every channel we consider the non-zero element as important and put a virtual surface around it and fined signed distance to it for every element of the bitmap.
The ambiguity for the force direction is solved by defining inner part of zero-surface as either inner or outer part while computing the SDF.

\subsection{Signed Distance Field}
\todo{should be explained?}

One of the ways to describe a set of geometrical points $\Omega$ within domain is defining distance to the boundary of $\Omega$ for every point of the domain. 
The values of such function are typically negative if the point is located within $\Omega$ and positive is outside.
In the end we define the Signed Distance Field as a metric space $X$ with metrics $f$, or \textit{signed distance function} :    
\[ 
f(x) = 
	\begin{cases}
			d(x, \partial \Omega) & \mbox{ if } x\in\Omega  \\
			-d(x, \partial \Omega) & \mbox{ if } x\in\Omega 
	\end{cases} 
	, \quad \mathrm{where} \; d(x, \partial \Omega) := \inf_{y \in \partial \Omega}d(x, y)
\]

If $\Omega$ is a subset of Euclidean space and has smooth boundaries, then the gradient of the metrics is constant at every point and satisfies Eikonal equation.

\[
 |\nabla f(x)| = \mathbf{1}, \quad \forall x \in X
\]

This equation describes the propagation of the wavefront started from the boundary of the $\Omega$ and its solution will describe travel time of the wavefront to every point in case of the constant speed of propagation which is equal to the signed distance function.
\medskip

There is a fast way to solve this equation for the points located on the regular grid called \textit{Fast Marching Method (FMM)}.
It is a graph traversing algorithm based on the Dijkstra's algorithm \ref{} and it's complexity is $O(M \log M)$ where $M$ is number of grid points.
There are plenty of FMM implementations available, and we chose to use python package \textit{ski-fmm} \ref{}, an part of SciKit framework that allows to obtain SFD as numpy array by calling only one function passing array describing zero-boundary to it.
\medskip

In this way we have a fast and easy way to describe our input images having all the pixels populated with meaningful values.

\subsection{Training Data Set}

One of the main goals of the work was to estimate the capabilities of such approach to produce the model having high accuracy, trained using as small train data sets as possible.
For 3D case preparing every data sample was taking more than a second for a relatively coarse resolution, which means that preparing data set, together with training of the network, will diminish the speed up gained by applying the inference system.  
\todo{get real performance}

To define the influence of chosen trained dataset on the resulting model and its performance, we chosen several of those with different number of samples and with diffident distributions of parameters within. 
One of the main questions was also how to chose the samples with respect the points where loads are applied for this case, so we created data sets each having different patterns for coordinate of force application point and trained models for those to see which one gives better model performance.
Among the patterns we chosen the next ones: a regular grid, checkered pattern, a sparse grid and force applied at the edges.

\todo{Add pictures of load layout}

 
 
\subsection{Post-processing}

The inferred outputs of the model demonstrate several recurring issues related to the material layout having incorrect physical properties.
One of the issues is material having the volume not equal to the one stated as input.
The other issue is material forming non-contiguous shapes, which is violating one of the requirement which is automatically met while conventional TO process.
There also might occur similar problem of the result not corresponding to some topological properties, like structural elements of the layout being not connected or not having physical meaning.
\todo{fix it if wrong}
\medskip

In order to solve these issues we decided to perform several post-processing operations on the output of the model.
We decided to apply for mathematical morphological operation in order to solve the over-mentioned issues.
\medskip

We use two basic operations, namely \textit{erosion} and \textit{dilation}.
The erosion is defined as 
\begin{equation}
A \ominus B = \{z \in E | B_{z} \subseteq A \}, \, \mathrm{where} \; B_{z}=\{ b+z | b \in B \}, \, \forall z \in E 
\end{equation} 
This essentially can be interpreted as cutting a layer constructed out of elements $B$ from every contiguous figure of the image.

The dilation is defined as 
\begin{equation}
A \oplus B = \{z \in E | (B^{s})_{z} \cap  A \neq \emptyset \}, \, \mathrm{where} \; B^{s}=\{x \in E | -x \in B \}
\end{equation} 
Similar to erosion, this could be interpreted as adding a layer constructed out of elements $B$ to every contiguous figure of the image. 
\medskip

In result, the post-processing sequence consist of the next steps:
\begin{itemize}
	\item \textit{Apply Morphological Opening}: Firstly we apply erosion of the image by small ball element $B$ and then we apply dilation of the image by the same element. In the result all domains of connectivity smaller than $B$ will vanish.
	\item \textit{Apply Morphological Closing}: Firstly we apply dilation of the image by small ball element $B$ and then we apply erosion of the image by the same element. In the result all gaps smaller than $B$ will be closed.
\end{itemize}

In case if the volume fraction of the result is larger that desired, which happened in practice most of the time, we try to reduce eliminate the elements that has the lowest probability to be occupied with material.
For that, we represent the resulting image as sparse matrix in form of coordinate list\ref{}, meaning the that matrix is read as list of tuples $(\mathrm{row},\mathrm{column},\mathrm{value})$ and could be easily sorted by value and delete the lower value until we reach a volume fraction value close to the desired.

In total, using this approaches we were able to bring the resulting model to a more physically feasible shape and to ensure that its volume has desired value.


\todo{explanation on operations, a pseudocode}

\todo{image before, image after}
 