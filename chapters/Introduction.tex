% !TEX root = ../main.tex

\chapter{Introduction}
\label{chapter:Introduction}

In this chapter, we describe principles and the application scope of Topology Optimization algorithm and explain the performance challenges of this method.
We present an approach to improve the performance of the method based on an Artificial Neural Network, justify the applicability and novelty of such approach and show ways it help to improve the performance of Topology Optimization. 
Finally, together with the explanation of Topology Optimization method, we give a description of Convolutional Neural Networks, their principles, and all their components.


\section{Motivation}

Topology Optimization (TO) is a mathematical method to optimize material layout within a given design space for given set of constraints, including loads, boundary condition etc. in order to maximize the performance of the system.
It is used for manufacturing in areas where usage of material or weight of elements is crucial, for instance, in aerospace engineering\cite{}.
Topology Optimization is an essentially computationally complex problem and various approaches to improving its performance are being researched.
\todo{find complexity of TO}
\todo{find real numbers on performance of TO, with hardware}
\medskip

The optimization is proceeded in iterations, for each of which the Finite Element Analysis is performed in order to assure that the result is correct and corresponds the physics of the problem, as well as it is possible to derive the functional gradient.
This step is the most expensive step of all and can be internally optimized to increase the speed of computation, however, the other aspect of performance tuning is reducing the number of optimization iterations made.
The convergence speed of the optimization is scarcely studied and is mostly analyzed in empirical ways.
\todo{convergence of TO}
\medskip

One of the ways to speed up the optimization process reducing the number of iterations is to start optimization with a certain educated guess of the material layout instead of the typical way, which is initializing the whole domain covered with the material.

\medskip
In the ideal case, the TO problem material layout could be initialized with an approximate solution which will require only a couple iterations to converge under the chosen criteria, in this way eliminating multiple initial steps of analysis.
The first step of the optimization typically change the layout drastically and result into a very coarse solution with a significant decrease of the objective function, but the value still stays much higher than for the desired outcome. 
\medskip

Furthermore, such approach allows reducing the computational domain of the problem by leaving out elements for which we know in advance that the material will not be present there. 
\medskip
\todo{revise motivation}

These initial steps could be made by an algorithm that is much less computationally expensive than the Topology Optimization, which will reduce time spent on FEM analysis. 
Despite TO results being possible only to predict heuristically, there exist some tendencies in the dependence between the Boundary Conditions and forces applied to the element. 
\medskip

As one of the possible solutions to find approximations of the TO outcome, we propose using Artificial Neural Network (ANN) approach.
The idea behind it is to build Machine Learning (ML) system able to infer a coarse result of a TO process based on the initial inputs for it.
The ML model has a form of ANN with parameters trained on a dataset which consists of the known cases of Topology Optimization problem, with inputs being pre-processed initial conditions of Topology Optimization and outputs being the outcome of TO, namely the final material layout design.
\medskip

In this work, we propose a design of an ANN system for fast inference of approximate optimal material layout for minimal compliance for elements with constrained volume under stress based on initial conditions, constraints, and parameters of the problem. 
We present the architecture of the problem, analyze the influence of dataset parameters and its representation on the quality of inference results as well as the general capacity of the system to improve the performance of Topology Optimization process.  
\medskip

Unlike many of the Deep Learning applications, the training dataset is principally hard to obtain, since every sample would be a pair of some initial conditional and the material layout, which is a result of an expensive TO process. 
One of the most important question in the work is the trade-off between the size and properties of the training dataset and the quality of the inferred result with respect to their accuracy and suitability for the initial layout for the TO process.
\todo{improve texting}
\todo{reduce vain stuff and increase size of the text}


\section{Previous Work}

The goal of this section is to give an outlook of the previous research on the possibilities to improve Topology Optimization performance.
In particular with consider using data-driven approach and the Machine Learning methods.
We also goal to overview the application of Machine Learning for the simulations in engineering, especially the cases dealing with spatial 3D data and inferring the results of some simulations.
\medskip

One of the recent work \cite{bibl:prevwork_pca} suggest applying dimensionality-reduction techniques for 2D optimization domains, in particular finding an eigen-vector representation using Principal Component Analysis\cite{} and treating the loading vectors as inputs for simple feed-froward neural network to obtain approximate result used as initial guess for the conventional TO process.
\medskip

The other work \cite{prev:prevwork_cnncfd} describes applying Convolutional Neural Network for prediction of approximate results for Computational Fluid Dynamics steady flow simulations. 
Authors suggest treating resulting velocity fields and representation of boundary conditions in an image-like fashion and propose an Encoder-Decoder CNN architecture for the model that will predict simulation outcomes based on pictures of initial conditions.
\todo{add on these works}
\todo{couple of article more}

\section{Topology Optimization}

Restrictions on the weight and amount of material used for a certain mechanical element is not uncommon in the engineering.
Various techniques allow finding optimal layouts of the element within the material that assures meeting of the requirement put on the physics of the element. 
\medskip

One of the approaches used for TO is Solid Isotropic Microstructure with Penalization (SIMP).
This method uses the idea of ``artificial density''\cite{to_simp}, where for every hexahedral element of discretized model a value of density variable $\rho$ is assigned.
The density plays the role of design variables in the optimization process and used to present the goal function as a weighted sum of element-wise compliance.
This approach has both well-established theoretical and empirical base and is easy to implement and interpret.
In the end, the TO problem within SIMP approach is formulated as follows:
\todo{revise formulation} 
%\begin{equation}
	\begin{align*}
		\underset{\rho}{\mathrm{minimize}} \quad & c(\rho) = \mathbf{f}^{\mathsf{T}} \mathbf{u} = \mathbf{u}^{\mathsf{T}} \mathbf{K}_{e}(\rho) \mathbf{u} \\
		\mathrm{subject \; to} \quad & \frac{V(\rho)}{V_{0}} = \alpha \\
		& \mathbf{K}_{e}(\rho) \mathbf{u} = \mathbf{f} \\
		& 0 < \rho_{\min} \leq \rho \leq 1
	\end{align*} 
%\end{equation} 

In other words, our goal is to minimize compliance function $c(\rho)$ subjected to the constraint of volume fraction $\alpha = \frac{V(\rho)}{V_{0}}$, where $V(\rho)$ is volume occupied by the material and $ V_{0} $ is total volume of the design domain.
The element stiffness matrix is denoted as $\mathbf{K}_{e}(\rho)$ and displacement and force vector are respectively $\mathbf{u}$ and $\mathbf{f}$.
Apart from the constraints on the volume fraction and the range of density values, the displacement vector, used to calculate the objective function, has to be physically correct, which means that one every iteration of the optimization process a Finite Element Analysis (FEM) problem should be solved to find $\mathbf{u}$ and this step is the most computationally expensive in the whole TO process.
\medskip

In order to update the density values on every iteration of the optimization process, the Optimality Criteria method is used \cite{to_simp, to_99line}.
The update rule read as following:
\todo{formulation}
\[ \rho^{new}_{e} = 
\begin{cases}
	\max(\rho_{min},\rho_{e}-\delta_{\rho}) , \; \mathrm{if} \; \rho_{e}B^{\eta}_{e} \leq \max(\rho_{min},\rho_{e}-\delta_{\rho}) \\
	\min(1,\rho_{e}+\delta_{\rho}), \; \mathrm{if} \; \min(1,\rho_{e}+\delta_{\rho}) \leq \rho_{e}B^{\eta}_{e}\\
	\rho_{e}B^{\eta}_{e}, \; \mathrm{if} \; \max(\rho_{min},\rho_{e}-\delta_{\rho}) < \rho_{e}B^{\eta}_{e} < \min(1,\rho_{e}+\delta_{\rho}) 
\end{cases}
\]
where $\delta_{\rho}$ is a non-negative increment of the design variable, the $\eta = \frac{1}{2} $ is a numerical dumping exponent coefficient.
The sensitivity value $B_{e}$ is updated by the optimality condition:
\begin{equation}
	B^{\eta}_{e} = \frac{-\partial c / \partial \rho_{e}}{\lambda \partial V / \partial \rho_{e}}
\end{equation}
Here we shall obtain the Lagrangian multiplier $\lambda$ by a bisection algorithm.
Finally, the sensitivity of the object is computed as 
\begin{align*}
	\frac{\partial c}{\partial \rho_{e}} & = -p(\rho_{e})^{p-1} \mathbf{u}^{T}_{e} \mathbf{K}_{0} \mathbf{u}_{e}\\
	\frac{\partial V}{\partial \rho_{e}} & = 1
\end{align*}

and used in the following update step.
The total optimization process could be visually described with \ref{fig:to_flow}.
\todo{revisit}

\begin{figure}[]
	\centering
	\includegraphics[width=0.6\linewidth]{images/TO_flowchart.pdf}
	\caption{Flowchart of the Topology Optimization process.}
	\label{fig:to_flow}
\end{figure}



\section{Convolutional Neural Networks}


Machine Learning approaches goal is to find a model that would interpolate some specific dependency of a nature, that could not be modeled explicitly, between based on the inputs and outputs(labels) samples present in the training data set.
\emph{Artificial Neural Network (ANN)} is a type of Machine Learning model for which a dependency sought is expressed as a composition of multiple operations, called \emph{layers} each having a number of parameters which values are find in the process of training.
Domains of these operations, as well as intermediate results images and generally the data with which an ANN model operates, are mostly spaces of matrices tensors over the real numbers field, and the results of the layer operations could be presented element-wisely, as a dependency of elements of input.
Such as element of intermediate result, together with the element-wise operation, is called a \emph{neuron}, after a biological neuron of a brain that is able to generate electrical impulse based on signals received from several other neurons, and such analogy historically inspired development of Artificial Neural Network approach by Frank Rosenblatt\cite{}.
The output of the neuron $y_{j}$ that takes input from $n$ neurons $x_{i=1,..,n}$ with weights $w_{i=1,..,n}$ and some activation function $\mathrm{f}$ is defined as $ y_{j}=\mathrm{f}(\sum_{i=1}^{n} x_{i}w_{i})$
\medskip
\todo{add examples of single FCL neuron? }

Convolution Neural Networks (CNN) is a type of ANN that primarily use of convolutional operations.
CNNs are specifically designed to work with images and image-like spatial data.
One of their unique features that distinguish them from other types of ANNs and make them suitable for these types of data is locality.
That means that output neuron is influenced only by a vicinity of neurons in the input.
The size of such vicinity is called \emph{local receptive field} of the neuron.
In CNNs every element of the next layer is achieved by sliding the local receptive field along the previous layer and applying a convolution operation.
\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/cnn_dem_gen4.pdf}
	\caption{Receptive fields of the neurons of the output}
	\label{fig:rec_field_demo}
\end{figure}
\todo{add captions, maybe add example for all phenomena(padding, stride etc)}

If it is important to have inputs and outputs of the layer of same size, it is possible to apply \emph{padding}.
That means that for the output elements close to the edge we span the receptive field outside of the size of the input, using virtual input elements with value of $0$. 
\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/cnn_dem_padding2.pdf}
	\caption{Convolutions with padding saves the size of the output}
	\label{fig:padding_demo}
\end{figure}
\todo{Convention: use elements of array OR neurons of the network OR pixels of image}
\todo{for such formulation -> output element correspond to middle of reception filed, change formula further!!!}
\medskip

The distance between two receptive fields, or a size of the sliding step,  is called \textit{stride} and could vary for different layers.
A convolution wit a stride larger that $1$ reduces the size of image and every element of its output encapsulates more unique influence of the input. 
%The larger stride will result in a smaller output layer with every element capturing more unique influence of the input layer, allowing to obtain a representation of the input in a lower dimension.
\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/cnn_dem_stride4.pdf}
	\caption{Stride size defines distance between two nearest windows of convolution.}
	\label{fig:stride_demo}
	\end{figure}
\medskip

The \textit{convolution operation} could be defined by obtaining a value of every element of the output image as a weighted sum of the values of input image elements of a region in vicinity of this output element.
In general form, we say that a \emph{convolution operation} on a $\mathrm{n}$â€”dimensional array of size $N_{1} \, \mathrm{x}... \mathrm{x} \, N_{n}$ with another array of same dimensionality and size $K_{1} \, \mathrm{x}... \mathrm{x} \, K_{n} $ gives result of an array of same dimensionality with size $ (N_{1}-K_{1}) \mathrm{x}... \mathrm{x} \, (N_{n}-K_{n}) $, 
% could be described as a mapping $\mathrm{f} : \mathbb{R}^{N_{1} \mathrm{x}... \mathrm{x} N_{n}} \rightarrow \mathbb{R}^({N_{1}-k_{1}) \mathrm{x}... \mathrm{x} (N_{n}-k_{n})} $ 
where every element of a resulting array is defined as a dot product of kernel and elements in a vicinity of corresponding element of the input.
In total, we can write down the operation as:
\begin{gather*}
\mathbf{C} = \mathbf{A} \ast \mathbf{B} \quad , \mathrm{where} \quad \\ \mathbf{A} = (a_{i_{1},...,i_{n}}) \in \mathbb{R}^{N_{1} \, \mathrm{x}... \mathrm{x} \, N_{n}} , \, \mathbf{B} = (b_{k_{1},...,k_{n}}) \in \mathbb{R}^{K_{1} \, \mathrm{x}... \mathrm{x} \, K_{n}},
\mathbf{C} = (c_{j_{1},...,j_{n}}) \in \mathbb{R}^{(N_{1}-K_{1}) \mathrm{x}... \mathrm{x} (N_{n}-K_{n})} \\ \quad \mathrm{and}
\quad c_{j_{1},...,j_{n}} = \sum_{i_{1}=1,...,i_{n}=1}^{K_{1},...,K_{n}} a_{j_{1}+i_{1}-\lfloor \frac{K_{i}}{2}\rfloor,...,j_{n}+i_{n}-\lfloor \frac{K_{n}}{2}\rfloor} \ast b_{i_{1},...,i_{n}}
\end{gather*}

For us, the input is used as a first operand and as second operand we use the array of weights.
The second array is called \emph{filter} or \emph{kernel} and is typically a parameter that should be trained. 
\medskip
\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/cnn_dem_opration2.pdf}
	\caption{A vizualization of a convolution operation.}
	\label{fig:convoper_demo}
\end{figure}
\medskip

The other important property of CNNs is \emph{parameter sharing}. 
It lies in the fact, that the kernel, which apply for every local receptive field to get the next element of the output, is the same for the whole layer. 
This property correspond to the nature of most of the images, which could be described as consisting of different features combined in larger entity.
In the process of optimization the optimal weights found describe these features that recur within the training data set and the intermediate output of layers after find the application of convolution show the representation of previous image in terms of these features.
\todo{add example(edge kernel))}
Furthermore, we have to train only the elements of the kernel that will be applied to every block of the pixels with some shift and the size of the kernel is usually much smaller than the size of the image.
That means that parameter sharing drastically reduces the number of model's parameters compared to the fully connected networks and such operations are beneficial to the performance of the model's training.
\medskip

One of the other operations that is used alongside convolutions in CNN is \textit{pooling}.
Essentially it is a reduction operation that averages values of pixels of a domain of the image into a single value.
This reduces the size of the image at the next layer and building a representation in a lower dimension space. 
Typically, average or max operations are used and the windows at which operation is applied do not overlap.
%slides so that there will be no overlaps between two neighboring windows, i.e. the stride of the slide is equal to the length of the kernel.
This could be generalized with a convolution operation with a stride equal to the size of kernel, with the difference being that the elements of such kernel should also be trained.
However, in many cases we could reduce the number of parameters to tain by fixing the weights of strided convolution kernel, for example the average pooling will be identical to te kernel values all equal to the inverse of number of its elements, without significant loss of accuracy\cite{}.
\medskip

%The filters trained in such a layer could be understood as features. 
%During the inference, the result of applying a convolutional operation to one of the channels denote a ...

On the other side, there are operation that allow increaing the size of the next layer, called \emph{deconvolutions} or \textit{transposed convolution}, which are, essentially, an interpolation of a previous lower-dimensional layer onto a higher resolution.
Every element of the input is element-wisely multiplied by the kernel and projected to the output, thus every element of the output is equal to sum of the elements of input multiplied by kernel of size $n$ in form $b_{ij} = \sum_{k=i-n+1,l=j-n+1}^{i,j} a_{kl}w_{i-k+1,j-l+1}$, which could be visually represented with \ref{fig:deconv_op}.
\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/cnn_dem_deconvop.pdf}
	\label{fig:deconv_op}
	\caption{Visualization of the transposed convolution operation.}
\end{figure}
For this formula the dependency of input on the output with given weights will have a form of standard convolution.\todo{should it be a ref?}
Combining with different length of the window stride it is possible to interpolate a previous layer to a layer of an arbitrary larger size. 
In our case, we apply transposed convolution with a stride of size $2$.
In such way, we reconstruct the result based on the low-dimensional representation of inputs using features found in the space of results.

%We also used a representation of all dimensionalities found before, adding additional layer after every residual block of the decoder.
\medskip


\subsection{Optimization}

The goal of the ANN training is to achieve a model that would predict the accurate results based on the various inputs.
For that, we need to estimate how wrong are the predictions $\hat{Y} = \{\hat{y}(x), \, x \in X \}$ based on known ground-truth pairs of inputs and outputs $ (x, y) \in X \times Y $ and.
We express the difference between the predicted on ground-truth results, as well as how generally wrong our model is, with the \emph{loss function}, which could be also called cost or objective function.
Because typically the outputs $y_{i}$ are taken from a finite-dimensional vector spaces, one of the typical way to build a loss function is to sum the distances between all pairs of $y_{i}$ and $\hat{y}_{i}$.
For example, we can take squared $L_{2}$ norms of the $y_{i}-\hat{y}_{i}$ errors and compute total Euclidean loss as $ L(\hat{Y}|Y) = \frac{1}{2}\sum_{\hat{y} \in \hat{Y}}(y - \hat{y})^{2} $.
The goal of the \emph{training} process is to find such weights, or values of model parameters, that minimize the value of the loss function.
\medskip

Typically, it is virtually impossible to analyze the dependency of loss function on parameters analytically, as well as to check all possible combination of weights values, because number of parameters is extremely large the dependencies are non-trivial. 
Due to that, the training is implemented through a numerical optimization, typically based on \emph{Gradient Descend} algorithm. 
The idea of the method is based on finding the numerical gradient of the loss function with respect to parameters to iteratively update them until some convergence criterion is reached, so that the loss function value is considered minimal for the set of found parameters values.
\medskip

We can express for the gradient descend algorithm as iterative process, where every step consists of finding derivative of loss function at the current point of parametric space $\nabla L(W_{t})$, which indicates the direction of the fastest growth of the function. 
Then, based on it value, a new point of parametric space, or updating the model weights, as $W_{t+1}=W_{t}-\alpha \nabla L(W_{t})$, where $\alpha$ is the \emph{learning rate}.
On next iteration the gradient is found for the updated point, and the process terminates when one of the user defined criteria is met, for example the change in the loss function is too small or the number of iteration exceeds the maximal amount.
\todo{add figured demonstrating loss function surface, GD process and variations}
\medskip

\subsubsection{Modifications of Gradient Descend an Techniques}

The classical gradient descend assumes that we could easily estimate the gradient of loss function at every point of the parametric space.
However, in case of machine learning, the gradient value would depend on multiple values for different inputs.
Essentially, in full form, to find a single value for a fixed set of parameters, it would require calculating and averaging gradient found for every sample in data set, which is practically infeasible.
To avoid that, the \emph{Stochastic Gradient Descent} is used, for which we use one or only a few samples, called a \emph{minibatch}, chosen randomly to calculate the gradient.
Besides being much faster, this simplification shows good convergence and is very popular as an NN optimizer\cite{}.

\begin{figure*}
	\begin{multicols}{2}
		\includegraphics[width=\linewidth]{images/parabolic_3d.pdf}\par
		\includegraphics[width=\linewidth]{images/nonconvex_3d.pdf}\par
	\end{multicols}
	\caption{Loss function forms. Left one is convex and has a single extremum. Right one is non-convex is ill-posed for Gradient Descend. }
	\label{fig:lossfunc_demo}
\end{figure*}
	
However, this method has multiple disadvantages.
The algorithm give guarantee to converge only for the convex goal functions and generally converge slow for ill-posed problems, with very slow evolution along the gradient.
For the non-convex shapes of the function there is a chance that it the algorithm will converge to a local minimum, without any possibility to escape and discover other regions of the domain.
Nevertheless, this approach is still being the most common way to optimize the parameters, especially if several modification are introduces as a remedy for the disadvantage.
\medskip

One of the simplest solution to the slow convergence is introducing a notion of a numerical \emph{momentum}.
It is inspired by the mechanical dynamics and now the optimization process is closer to simulation of a particle motion with the goal function playing a role of the potential field.
The update rule has an additional velocity term $V$ which adds certain inertia to the way parameters change, pushing the virtual particle more along the gradient. 
One of the gradient descend based method that use the idea of momentum is \emph{Nesterov Accelerated gradient (NAG)} and it shows better convergence properties compared to simple gradient descend.
The algorithm is interpreted as follows: 
we take an intermediate point in the direction of the momentum and calculate gradient for it.
Than we use the gradient value to update both the parameter values and the momentum.
\begin{align*}
	V_{t} & =\mu V_{t} - \alpha \nabla L (W_{t}+\mu V_{t}) \\
	W_{t+1} & = W_{t+1} + V_{t+1}
\end{align*}

Apart from inability to find the region of global minimum, the other issue is fine-tuning the value of optimal parameter.
With a large learning rate the step of the optimization stays also large, which leads to oscillating behavior of optimizer that constantly oversteps the minimum.
To improve the accuracy during the later iterations the learning rate should be decreased with time, which is often done manually, with some predefined law. 
However, a more general way, would be to find learning rate adaptively, based on the pass made by the optimizer in the parameter space.
The \textit{Adaptive Gradient Algorithm (AdaGrad)} uses sum of squared, thus non-negative, gradients over time $G_{t+1} = G_{t}+\nabla L(W_{t})^2$ to define the learning rate.
In this way the learning rate is large only while the historical gradient is small, and growth small while the gradient accumulates its value $ W_{t+1} = W_{t} -\mu \frac{\nabla L(W_{t})}{\sqrt{G_{t+1}}+\epsilon} $.
Furthermore, here every parameter has, in the end, its own learning rate, which allows keeping total precision and convergence speed at good rate.
Finally, the case of division by zero should be also prevented. 
\medskip

One of the most popular optimization algorithm treats well both finding global minimum and fine-tuning, is \emph{Adaptive Moment Estimation (Adam)} with the following update rule.
\begin{align*}
	M_{t+1} & = \beta_{1}M_{t} + (1-\beta_{1})\nabla L(W_{t}) \\
	V_{t+1} & = \beta_{2} V_{t} + (1-\beta_{2})\nabla L(W_{t})^2 \\
	W_{t+1} & = W_{t} - \mu \frac{\sqrt{1-\beta_{2}^{t+1}}}{1-\beta_{1}^{t+1}} \frac{M_{t+1}}{\sqrt{V_{t+1}}+\epsilon}
\end{align*}		

Adam combines all mentioned over approaches and generalizes AdaGrad method.
It uses second order momentum to adapt learning rates, a first order moment as an inertia term and bias correction multiplicative constant to prevent from update steps being close to zero.
\todo{ references. too long/too sloppy?}

\subsection{Backward Propagation}

The optimization process requires knowing values of the gradient of loss function with respect to parameters space.
As we told, the form of the loss function is unknown, defined on high-dimensional prameter space, dependent on data, and in principle infeasible to analyze.
Thus, it is only possible to find it finite number of point that are of interest based on some numerical considerations.
\todo{i don't like it either}

\medskip
One of the idea that made ANNs fast and easy to implement is \emph{automatic differentiation}.
Since we describe the dependency between output and output of the network as a sequence of simple data transformations like convolutions or activation, called layers, it is possible to determine the gradient of every such operations and save it.
Having the total transformation and applying the chain rule, we can easily express the total gradient using the atomic derivatives of every transformation, and the process of finding is called automatic differentiation. 
\todo{formulas to demonstrate?}

\medskip
In its turn, the process of plugging the values on every iteration of optimization in order to find loss function gradient value and update the parameters is called \textit{backward propagation} o \emph{backpropagation}.
It is an essential principle of ANNs and many modern software frameworks offer mean for automatic differentiation and back propagation allowing easy model set-up and good performance of its training\cite{}.


\subsection{Activation Function}

For now, all the operations performed on the input was described as linear.
However, not every dependency could be described and approximated as linear and such simple model could easily miss complex patterns.
In order to build a more general and simple model, non-linear \emph{activation functions} allied to the output of the layer could be introduced.
\medskip

One of the most famous activation function is the \emph{sigmoid} function.
This is a deterministic function $ \sigma : \mathbb{R} \rightarrow [0; \, 1] $ and reads as $ \sigma(x) = \frac{1}{1+e^{-x}} $.
\todo{add figure}
One of the issues of sigmoid function is that it is not zero centered.
If the input values of the neuron always have the same sign it will lead to issues with the weight updates during the back-propagation process, since the values of the gradient will also always have the same sign.
\medskip

The other popular non-linear activation function is hyperbolic tangent which is a mapping $ \sigma : \mathbb{R} \rightarrow [-1; \, 1]$ and equal $\tanh(x)=\frac{2}{1+e^{-2x}}-1$.
It is very similar to the sigmoid, but due to the symmetrically of the range, it is zero-centered.
\medskip

As it is seen from the graphs of functions, for the values of the tails of both of the sigmoid and hyperbolic tangent the value of the function is very close to either $0$ or $1$.
This is called the \emph{saturation problem} and it is expressed in the fact that for the most of the domain the gradient of the function is very close to zero.
The gradient of the activation function influence the gradient of the cost function which leads to very slow convergence rate of the training process.
This phenomenon, also called the \emph{gradient vanishing} is one of the reasons why $\mathrm{sigm}$ is rarely used as an activation function nowadays, while $\tanh$ still has some use.
\medskip

The third family of activation functions is \emph{Rectified Linear Unit} (ReLU) and equal to thresholding the input value at zero $f(x)=\max(0,x)$. 
\medskip

It is the most widely used activation function in the last years and it shows better a convergence of Stochastic Gradient Descend than previously mentioned ones and it computationally very simple.
The main disadvantage is that there is a chance that a large bias term will be calculated which will lead to so-called \emph{"dying ReLU"}. 
That means that the gradient of the operation stays zero and the training process is stopped without any possibilities to recover. 
\medskip

The common solution to this problem is to replace putting to zero at the native part of the domain with the function with small negative slope, so that gradient will never be zero. 
This type of activation function, shown in \ref{fig:act_func} is called parametric ReLU (PReLU) and defined as  
\[ f_{PReLU}(x) = 
\begin{cases}
	\alpha x \, :x<0 \\
	x \, : x \geq 0
\end{cases}
\]

One of the further notifications is a \emph{Exponential Rectifier}.
Like the ReLU family activation function it avoids gradient vanishing, is enhances normalization and shows a speed up for training\cite{bibl:elu}, however is slightly slower during propagation. 
\[ f_{PReLU}(x) = 
\begin{cases}
x \, :x<0 \\
a(e^{x}-1) \, : x \geq 0
\end{cases}
\]

\begin{figure}[]
	\centering
	\includegraphics[width=0.7\linewidth]{images/activfunc_4.pdf}
	\caption{Comparison of the different activation functions. Blue is $\mathrm{sigmoid}$, red is $\mathrm{tanh}$, yellow is $\mathrm{PReLU}$ and purple is $\mathrm{ELU}$.}
	\label{fig:act_func}
\end{figure}

\todo{revisit, finish up}
\todo{actually, a definition of channel should be given as well}

In our work, we apply another two advanced techniques used with activation functions.
The first one is called \emph{concatenated activation} wich we used together with ELU.
It is inspired by the idea that the negative values propagated through network could be as important as positive ones, and, because they are treated by activation asymmetrically, they should be considered separately as well\cite{bibl:concat_rectif}.
That means that at the activation step we should map every channel into two channels, the identical one and the negated one, and apply the activation function to them separately, thus considering the more general picture.
\medskip

The other one is \emph{gating} and is based on doubling the number of channels at the convolution and then treating the groups of channels separately.
We apply a sigmoid function only for a half of channels and then multiple the results element-wisely with the leftover channels.
In this way, the convolution has two times more parameters comparing with a regular convolution leading to the same size of output tensors.
Doubling number of channels prevents gradient vanishing since during the back-propagation the gradient will have both direct and sigmoid components\cite{bibl:conv_gating}. 

\subsection{Regularization}

One of the common problem for then Machine Learning models is over-fitting, or low ability to generalize which occurs when the model performs very well on the training set but shows bad results with the unknown input data while evaluation.
\medskip

There are several ways to facilitate the ability of the model to generalize, and one of them is to put some restrictions on its parameters.
One can decree the number of parameters, which would lead to a simpler, and thus more general, model.
The other is adding a penalty term, which depends on the parameters' values, to the loss function, as $L(x|w) = L_{0}(x) + R(w)$ , which would tend to convergence to smaller values of the wights after the optimization. 
% \[
%	L(x|w) = L_{0}(x) + R(w)
% \]
Typically, with large values of parameters means the model fits well to the known data samples, which in its turn means that model leaned the effect of the noise in the training dataset, and lower wights make the outputs change less with the small change of the inputs (hence the model is more stable).
\medskip

\subsubsection{Common Regularization Techniques}

One of the most common types of the regularization are $L_{2}$ and $L_{1}$ regularization.
For the former one, the regularization term is $L_{2}$ norm of the model's parameters and could be expressed as $R=\frac{1}{2}\lambda w^{2}$ where $\lambda$ is a regularization parameter.
For the letter it is respectively an $L_{1}$ norm of parameters $R=\lambda |w|$.
In the result of the optimization, the $L_{2}$ regularization will lead to weights shrinking proportionally to their value, as $\frac{\partial \frac{1}{2} \lambda w^{2} }{\partial w} = \lambda w$, whereas $L_{1}$ shrinks them as constant, $\frac{\partial \lambda |w| }{\partial w} = \lambda \, \mathrm{sgn}(w) $.
For the latter one, the weight tends to lower fast and become sparse as many of them are put to zero value in the end, which allows explicit feature selection.
However, if there are no special restrictions on the number of parameters, typically $L_{2}$ regularization is used as it giver better results than $L_{1}$.
\medskip

The other way to deal with over-fitting is called \emph{dropout} and can be used together with the $L_{1}$ and $L_{2}$ regularization.
It is based on omitting of random neurons on each step of the training of the model.
Typically, an approach of choosing any neuron with the probability of $50\%$ is used so at every step we have a different combination of neurons.
In the end, we change the structure of the network during the learning process and the resulting network is equal to an average of several models, which reduce the effect of over-fitting of every single network.

Finally, one of the most common approach to avoid over-fitting is \emph{early stopping}.
The optimization process happens in iterations and usually end when the maximum number of iterations is reached.
However, it is possible to choose a different convergence criterion, for example reaching the optimum of validation error.
During the optimization the model tries to fit the training data samples as close as possible and can end up showing bad results for new data.
To avoid getting over-fitted model one can validate model during the process of training, inferring results on a data-sample that were not used for training and analyzing the validation error.
It could be performed once in several iterations of optimization and the start of validation error growth usually indicates over-fitting.
An similar approach would be to achieve several models  whir underwent different number of imitation of iterations and choose one with the lowest validation error.



