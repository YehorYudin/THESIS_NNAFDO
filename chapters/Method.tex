% !TEX root = ../main.tex

\chapter{Method and Implementation}

In this chapter we will describe the implementation aspects of the work.
Here description of the ANN architecture and its reasoning, the general pipeline of computations and data manipulations applied to set up the system creating data that helps the TO performance improvement and ways for the system analysis, as well as the software used for its creation.
Also we will talk about the aspects of choosing and preparing data for model training, as well as data pre-processing and post-processing.
\todo{what did I just write}
\medskip

%In this section we describe the important implementation aspects including the technologies and techniques used, the structure of the code and the main functions logic.


\section{ANN Architecture}

The model used by us to describe the dependency between boundary conditions and the resulting material layout is a CNN which structure relates to the class of \textit{encoder-decoder} architectures.
This network the fits for the cases when input and outputs are of the same size and consists of two groups of data transformations: encoding and decoding.
\medskip

The former one, encoding, goals to find a representation of the input in form of prominent features.
It consists of sequential application of size-preserving convolutions and pooling convolutions with increasing amount of features. 
After application of several blocks of such operations, inputs transform from a large image with few channels into many-channel image of very small size, which we call the low-dimensional representation. 
In our case, an image of $4$ channels transforms into $128$ different representations. 
\medskip

In our architecture, the encoder consist of nine blocks of convolutions, 
each training in a residual form.
A \textit{Residual Network} is a type of network, where an atomic part of transformation is defined in form $F(x) = H(x) + x $, which means the gradient would describe the change of the residual between the original and transformed data\cite{bibl:resnet}.
This allows to increase accuracy having smaller amount of layers and to speed up the training process.
\medskip

At every channel we apply two convolutions.
After the first one we apply the activation function and dropout on the phase of training for the sake of regularization, but not in the final model.
\medskip

At this point we also apply an activation function of \textit{Concatenated Exponential LU (cELU)}
Here we apply one of the techniques, that is often used together with activation function, namely \textit{concatenated} activation.
It is inspired by the idea that the negative values propagated through network could be as important as positive ones, and, because they are treated by activation asymmetrically, they should be considered separately as well\cite{bibl:concat_rectif}.
That means that at the activation step we should map every channel into two channels, the identical one and the negated one, and apply the activation function to them separately, thus considering more general picture.
\medskip

After the second convolution, we apply \textit{gating}, a different form of activation.
That means we double the number of channels at the convolution and then treat the groups of channels separately.
We apply a sigmoid function only for one half of channels and then multiple the results element-wisely with the leftover channels.
In this way, the convolution have two times more parameters comparing with a regular convolution leading to the same size of output tensors.
Doubling number of channels prevents gradient vanishing, since during the back-propagation the gradient will have both direct and sigmoid components\cite{bibl:conv_gating}.
\medskip

Every second block has stride for its second convolution equal to 2, which increase the size of the output in two times in every direction.
Here we also apply average pooling and pad empty channels at the residual channel to fit the new size of the image.
In total, the residual block of the encoder is represented visually with \ref{fig:res_block}.
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{images/Residual_Block2.pdf}
	\label{fig:res_block}
	\caption{A flowchart for residual block used for the encoder part of the network. Dashed denotes dropout applied only during the training phase. Purple denotes down-sampling operations applied every second block. Blue denotes convolution layers with weights to be trained.}
\end{figure}
\medskip

The latter part of the network, decoding, consist of several blocks of operations of normal convolutions and \textit{deconvolutions} or \textit{transposed convolution}, which are, essentially, an interpolation of a previous lower-dimensional layer onto a higher resolution.
Every element of the input is element-wisely multiplied by the kernel and projected to the output, thus every element of the output is equal to sum of the elements of input multiplied by kernel of size $n$ in form $b_{ij} = \sum_{k=i-n+1,l=j-n+1}^{i,j} a_{kl}w_{i-k+1,j-l+1}$, which could be visually represented with \ref{fig:deconv_op}.
\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{images/cnn_dem_deconvop.pdf}
	\label{fig:deconv_op}
	\caption{Visualization of the transposed convolution operation.}
\end{figure}
For this formula the dependency of input on the output with given weights will have a form of standard convolution.\todo{should it be a ref?}
Combining with different length of the window stride it is possible to interpolate a previous layer to a layer of an arbitrary larger size. 
In our case we apply transposed convolution with a stride of size $2$.
In such way, we reconstruct the result based on low-dimensional representation of inputs using features found in the space of results.
%We also used representation of all dimensionalities found before, adding additional layer after every residual block of the decoder.
\medskip

One of the issues of the transposed convolution is occurring checkered error pattern in the output.\todo{should it ba ref or a picture with previous results?}
This is caused by the fact that for kernel size being non-multiple of stride, different output elements have different number of input elements influencing them, which results give different magnitudes for some output coordinates of certain multiplicity.
To mitigate this effect we applied kernels of size $4 \times 4$, since the stride we used during up-sampling is $2$ in order to increase the size of channel two times in each dimension.
\medskip

After every transposed convolution, that up-samples the data propagating through network, increasing the size of the channels and decreasing the number of channels, we add a residual block of convolutions which correspond to the reconstruction of the image using features in the space of outputs.
Every such block is similar to the one used for the encoder, apart from using \textit{shortcut connections} from the previous layers.
That means that every block has an additional input, namely the result of the encoder layer that has same output size as current decoder layer, and after bringing layers to similar dimensionality we sum them.
This enhance the preservation of general geometrical properties of the input and facilitates the training process \cite{bibl:shortcuts}.
The shortcut image have multiple channels, and to reduce them we apply \textit{$1\times 1$ convolutions}.
They are, essentially, a weighted sum of input elements along channels only, and without spatial kernels, thus they change number of channels but keep their size. 
Also, we need to restore the size of the image after the up-sampling, because the striding technique will lead only to sizes multiple of lower layers, and we use the size of the shortcut image for that.
The encoder residual block could be easier understood using\ref{fig:res_block_upc}.

\medskip
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{images/Residual_Upconv0.pdf}
	\label{fig:res_block_upc}
	\caption{A flowchart of the residual block used in the decoder part of the network.}
\end{figure}

One of the tweaks we required to build a model for our case allowed the network having a scalar input as well.
Since we want to enforce the output having the desired non-empty volume fraction, we used its value as input as well.
We tiled a matrix of size of the lowest-dimensional representation with values of volume fraction and added one more channel at the bottleneck of the network. 
\medskip

After the sequence of four transposed convolutions, each followed by a residual block with a shortcut, we apply a single convolution layer that with one kernel to decrease the final channels dimensionality. 
In the end, based on the $128$ channels of low-dimensional representation, we obtain a final of single channel of size equal to the TO problem domain.
The total architectural of our model could be visually with\ref{fig:arch_1}.
The architecture for 3D case is very similar to the 2D one, with every convolution replaced with a corresponding \textit{3D convolution}, where channels and kernels are cubes and summation happens across $3$ dimensions.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{images/Architectue_resnet_cone.pdf}
	\label{fig:arch_1}
	\caption{The data flowchart of the model, describing the CNN's architecture. Here the sizes are shown for the $32 \times 32$ domain resolution, most commonly used by used, however we used only one.}
\end{figure}


\subsection{Loss function}

The optimization, performed during the training, goals the minimization of the loss function.
Thus, one of the most important steps during the design is defining a proper loss function which will precisely encapsulate our requirements on the results inferred by the model.
\medskip

Our idea of the result produced by the model is a bitmap describing material density layout being as close to the result of conventional TO process as possible.
In this way, we have to minimize the difference between the known optimized material design for given input and the result inferred as model.
However, the difference between two arrays of values is conventionally is not easy to describe with a single scalar and typically the influence of a single element of output is low, which would make the optimization process extremely slow.
\medskip

In order to tackle all these issues, we formulate our problem as a classification problem for every single element of the output channel for given input. 
We treat our reference, ground truth, image of layout as a bitmap for which every element can belong to one of two classes, namely occupied by material ($1$)  or not ($0$), for which we should threshold our results of known TO problems.
In such formulation, the goal of our system is to classify every element of output bitmap based on the inputs and give the confidence for the inferred classes.
Thus, for a single input our model performs binary classification for multiple outputs.
This means, that by discretizing the problem we reduce the information flux over the model we significantly increase the speed of the model training.
\todo{generally written very uncertainly... plus good to compare training time for different losses}
\medskip

In this way, we define our loss function as a sum over every output pixel of logits of binary cross-entropy between the output of the model and reference image.
\todo{ add a cite ,description is also so-so}
The loss function term has the next form:
\begin{equation}
 L_{CE}(w|x, y, \hat{y}) = -\frac{1}{N} \sum_{y_{e}|y=(y_{e})}^{} [ y_{e} \log \hat{y_{e}} + (1-y_{e}) \log (1-\hat{y_{e}})] , \, \mathrm{where} \, \hat{y_{e}} = \frac{1}{1+e^{-\hat{y}^{out}_{e}}}
\end{equation}
\medskip

Furthermore, in order to prevent over-fitting we incorporate a regularization term in form of a $L_{2}$ norm of all trained matrix parameters, multipled by a small constant $\alpha_{reg}$
\todo{describe regularization? how detailed or leave out?}
\begin{equation}
 	L(w) = L_{CE} + \alpha_{reg} R_{L_{2}}, \, \mathrm{where} \, R_{L_{2}}(w) = ( \sum_{i}^{} |w_{i}|^{2} )^{\frac{1}{2}} 
\end{equation}
\medskip

One of the clear requirements defined by the TO problem itself is the resulting layout having a fixed volume. 
Since the closeness to the reference data does not implicitly put any restriction on this criterion, we decided to use several ways to facilitate the inferred layout having desired volume.
One of the ways used in the system is adding a volume fraction penalty term to the loss function.
For this, we calculate the volume of the inferred result as a count of the pixels for which the probability to be occupied is higher than threshold, took the ratio with the volume stated for the reference problem and add the absolute difference with the desired volume fraction to the loss function with some small multiplicative constant $\beta_{V_{f}}$.
\todo{ more detail! general formula!}
\begin{equation}
	L(w) = L_{CE} + \alpha_{reg} R_{L_{2}} + \beta_{V_{f}}  L_{V_{f}} , \, \mathrm{where} \, L_{V_{f}} = \frac{1}{N} \sum_{i=1}^{N} |V_{f}(\hat{y}_{i}) - V_{f}^{\ast}|
\end{equation}


\subsection{Hyperparameters}

The model had a number of hyperparameters, including:
\begin{itemize}
	\item batch size
	\item volume fraction penalty constant
	\item regularization parameter
	\item dropout probaility 
	\item learning rate
\end{itemize}

The number of training iteration was also treated as a hyperparamter.
This also relates to the data set chosen to train the model, especially its size, and a particular form of the loss function.
architecture.
\todo{fix description and list}
\medskip

These hyperparameters differ from the weights of the layer of the model, because of their special nature ad because it is not possible to find a gradient of the loss function with respect to them.
However, chosen them is important to find the best model, and in order to find them \textit{hyperparameter optimization} is used.
\todo{explain hz-pam optimization?}
\medskip

We chosen key values of the hyperparameters, trained model for cases when the hyperparameters are equal to sets of those values and checked the evaluation parameter to find the best model.
\todo{???}

