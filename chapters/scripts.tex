% !TEX root = ../main.tex

\section{Implementation}


The system was designed using the Tensorflow framework. 
\todo{should it be a library?}
It is built in form Python program calling to the Tensorflow Pyhon API.
\todo{how to put it? description and name should be consistent everywhere}
We used 1.8 API version of Tensorflow run on GPU using CUDA 9.0 and CuDNN 7.0.
The main reason of the choice of the Tensorflow and python was the easy and fast way of building, modifying and understanding the code together with little overhead of
creating a program that able to be run using GPU. 
\medskip

\subsection{Tensorflow}

Tensorflow is an open source library under Apache $2.0$ license for high-performance numerical computation\cite{tensorflow_main}.
It provides API for multiple languages, including Python and is easy to deploy on multiple platforms, including Nvidia GPUs.
It was initially developed by Google's AI Organization and primarily supports the development of Machine Learning systems.
\medskip

Tensorflow requires a definition of the pipeline of data transformations in the code.
This definition is made in a functional manner using one of the high-level languages API and includes various operations on numerical and other types of data, as well as easy means to update and optimize variable parameters using backpropagation. 
While definition, the programmer can specify the device on which a specific operation will be performed and specific variable will be stored, as well as which variable to initialize and which to read at the end of the pipeline. 
During the pipeline descriptions user specify the placeholder for the data over which operations are performed. 
These placeholders are typically described as multi-dimensional arrays of certain data-type called \emph{tensors}.
The Tensorflow takes care about compiling the optimized code described using the API and the deployment of the code and data on the specified device. 
Then the user can run the defined operations for a concrete data using the mapping of the real input values with the input placeholders, the queue runners or iterators of some user predefined data set.
In such manner, one is able to define loops in which parameters of the model are trained as well as to use the model to infer results for certain input.
Tensorflow allows saving trained model parameters in an easy way, which in the end allow deploying the working model on a different model using only the script describing the pipeline and the file with model parameters values.
The other benefit of the Tensorflow is \emph{Tensorboard} utility with a web-based interface, which allows runtime observation over the system status with which user can see current values of any variable, like the value of loss function or intermediate result without almost any intrusion in the code.
\medskip

Such design of the framework with API in multiple languages together with intrinsic optimization for high performance at multiple platforms, as well as convenient tools for easy deployment and control over the system makes Tensorflow extremely useful for many scientific and industrial applications.

\section{Training Script}

In order and to quickly infer results that could be easily used for further TO and to proceed various experiments on performance, a software framework was created. 
The requirements of the software system were to be able to take various known cases of TO problem solutions, train the best CNN inference model, and create approximate solutions for the new TO problems.
It was implemented in a functional way as a sequence of scripts and has the view in the form of following algorithms presented in the pseudocode. 
\medskip

The first part of the system is the responsible for automatic data set generation.
The outcome of a single run describe in \ref{alg:data_gen} is a number of datasets populated by data sample that could be directly fed to the network for the further training or evaluation.

\begin{algorithm}[H]
\caption{The dataset generation process}\label{alg:data_gen}
\begin{algorithmic}[0]
	\ForAll{TO problem parameter values combination in a grid}
		\State Generate Data Sample i.e. resulting material layout
		\State Create sample
	\EndFor
	\ForAll{Sample} 
		\State Augment Data
		\State Parse case name
		\State Create Initial Condition models for inputs i.e. $ BC,F_x,F_y $
		\State Pre-process Inputs i.e. create SDFs
		\State Combine Inputs and form network-fedible sample
	\EndFor
	\ForAll{Dataset sampling scheme}
		\State Form Training Dataset
	\EndFor
	\State Create Evaluation Dataset
\end{algorithmic} 	
\end{algorithm}
\medskip

After we obtained different training datasets in a proper format, we can start training ANN models.
The process of training the models is described as the following algorithm\ref{alg:network_training}. 
\begin{algorithm}[H]
	\caption{Procedure of CNN models training}\label{alg:network_training}
	\begin{algorithmic}[0]
		\ForAll{H-P values combination $h=(h_1,...,h_n)\in$ the H-P values grid $ H_{1} \times ... \times H_{n}$ e.g. (Training Data Set, Number of training steps)}
			\State Create Batch Queue
			\State Set up Architecture and Loss function 
			\While{Current number of training iterations not exceeded}
				\State Get batch from the Queue and read the input samples
				\State Propagate the input forward and infer the result
				\State Find the loss function value and propagate gradients backward
				\State Update parameter values
				\State Save the state of the parameters and the loss function
			\EndWhile
			\State Save the parameter value checkpoint
		\EndFor
	\end{algorithmic}
\end{algorithm}
\medskip

After that, we evaluate the created models and obtain the MSE measures.
We chose the best model based on this parameter and, in our case, use it to infer results that will be used as an input for the IDeAs in order to evaluate how we can speed up the TO solution. 
The evaluation is described in the next algorithm\ref{alg:network_eval}.
\begin{algorithm}[H]
	\caption{Procedure of CNN model evaluation}\label{alg:network_eval}
	\begin{algorithmic}[0]
		\ForAll{Every model checkpoint}
			\State Load Evaluation Dataset
			\State Set up the Architecture
			\ForAll{Evaluation Samples in Evaluation Dataset}
				\State Infer results and find loss function
				\State Record data for further evaluation e.g. MSE and inferred model
			\EndFor
			\State Find evaluation measures value, e.g. ($\mathrm{MSE_{avg}}$) and heatmap
		\EndFor
		\State Choose the best performing model	as $\mathrm{argmin}_{model} MSE_{avg}$
		\State Use the model to generate results for the test set
		\If{3D case}
			\ForAll{Chosen test subset results}
				\State Post-process i.e. smooth and reach desired volume
				\State Initialize the TO model
				\State Evaluate the problem size and time to Optimize
			\EndFor
		\EndIf
	\end{algorithmic}
\end{algorithm}
\medskip

In the result, we give an estimation of how accurate is the best model for the current experiment set-up and how does it fit to facilitate the overall TO process.  
