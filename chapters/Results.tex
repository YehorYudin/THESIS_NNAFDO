% !TEX root = ../main.tex

\chapter{Results}
\label{chapter:Results}

In this chapter, we will describe the tests on model training, both for 2D and 3D domains, and results of their evaluation.
We analyze the dependency of the model performance based on the data set properties, as well as ability to interpolate results for different variable parameters, and ways to prepare the model with the best performance based on several criteria. 
The model is evaluated with respect to the accuracy of its results as well as its ability to improve the TO process is analyzed.
\medskip

The models for all cases are trained using GPU, which is also used to generate samples and measure TO speed-up and for 3D cases.
The graphics card used is a single Nvidia GeForce GTX 1080 Ti, with 3584 CUDA cores. 


\section{Results for 2D case}

%Initially, it was decided to create an architecture and system to treat 2D cases of TO problems.
During the first practical part of work, only the 2D cases of TO was considered.
Creation of architecture for 2D domains and analysis of its performance was an important step that proved a concept of such ANN approach for TO field.
The decision to perform 2D analysis was also dictated by availability and speed of dataset preparation, as 2D TO solutions takes significantly less time than 3D. 
For a 2D case, we could generate large training dataset with more variable parameters and experiment with different representations.
The dimensionality of the data also influenced the number of model parameters, which means that training a single model took much less time and it was possible to introduce changes or fine-tune the model.
\todo{move to approach or data?}

%The results of the model evaluation, including the hyperparameter optimization, shows next tendencies. 
\medskip

\subsubsection{Test I: Influence of training dataset size and sampling pattern}

One of the main tests was to find the influence of the training dataset properties on the model performance.
We performed evaluations for models trained with datasets of different size and of different sampling patterns, described in ``Data Preparation'' chapter.
The goal of the test was to find the dependency of the validation accuracy on the dataset size as well as to choose the best training dataset sampling pattern.
\medskip
 
After some preliminary research, we decided to take three different sampling schemes and three with three different sampling steps for each. 
%The most accurate model was produced by the training dataset with the domination of problems where loads were applied on the edge of the domain.
%This type of datasets shows slightly better results compared to others, even with smaller amount of training samples.
We trained models for various numbers of iterations and observed the saturation of validation error after some point ($10^4$ training iterations) as its shown on figure \ref{fig:mse_vs_steps}.
However, the case with domination of problems where loads were applied on the edge of the domain shows slightly better results compared to other with smaller amount of training samples.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.85\linewidth]{images/plot.pdf}
	\caption{The dependency of model validation error on the numeber of training steps.}
	\label{fig:mse_vs_steps}
\end{figure}
We also observed different patterns in the validation error heatmap.
The longer the training the less uniform was error with respect to load application point for the test case. 
For the coordinates of load application and their vicinity the error value became very low and for other points it increased. 
However, as it seen from the validation error graph \ref{fig:mse_vs_steps} the average error stays almost the same.
The validation technique requires more general approach, that will consider test set properties uniformity of error on the set.

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{images/hitmap_e16_15k_avg.pdf}
		\caption{Number of iterations $15,000$}
		\label{fig:heatmaps_overfit_1}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
		\centering
		\includegraphics[width=1\linewidth]{images/hitmap_dpcr13-40k_small.pdf}
		\caption{Number of iterations $40,000$}
		\label{fig:heatmaps_overfit_2}
	\end{subfigure}
	\caption{Validation error heatmaps after different amount of training steps. With training the error distribution based on become less uniform. The error becomes more dependent on how close are known cases used in the training.}
	\label{fig:heatmaps_overfit}
\end{figure}

\medskip
The dependency on the size of the training set, as shown with Figure \ref{ig:mse_vs_size}.
The validation error decreases almost equally for every sampling pattern and the dependency is close to inverse proportional.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/plot_SIZE_O.pdf}
	\caption{The dependency of model validation error on the training dataset size.}
	\label{fig:mse_vs_size}
\end{figure}

\medskip

\subsubsection{Test II: Loss function form and volume recovery}
The next test was performed in order to find out the capabilities of the approach to restoring proper volume fraction of the material. 
For this, we trained model with different loss function: 
\begin{itemize}
	\item Without volume fraction treatment $L_{(1)}$ as in equation \ref{eq:loss_reg}
	\item With additional penalty term as $L_{(2)}=L_{(1)}+\beta L_{V_{f}}$ as in \ref{eq:loss_vfpen}
	\begin{itemize}
		\item for a different multiplicative constant $\beta$ for the term
	\end{itemize}
\end{itemize}
We took training dataset with models of variable volume with three different volume fraction values.
For the evaluation, we used datasets composed of both models with known volumes and with models of volume fraction not used during the training.
We evaluated both accuracy of the model and the accuracy of volume value as a separate parameter.
The results show that considering volume fraction at loss function improves the accuracy of the model, as one can see from the table \ref{tab:vf_full} and \ref{tab:vf_full_vf}. 
\begin{table}[h]
	\begin{center}
\begin{tabular}{ |c|c|c|c|c| }
	\hline
	Loss vs. $MSE(Var)$ & $V_f=0.2$ & $V_f=0.3$& $V_f=0.5$ &  $V_f=0.3$, res. $28\times 28$\\ 
	\hline
	$L_{(1)}$ & 0.063(0.0001) & 0.072(0.0012) & 0.097(0.0013) & 0.168(0.006)  \\
	$L_{(2)}, \beta=0.01$ & 0.060(0.008) & 0.070(0.0013) & 0.10(0.001) & 0.156(0.006)\\
	$L_{(2)}, \beta=0.2$ & 0.058(0.0007) & 0.068(0.0009) & 0.092(0.001) & 0.162(0.005) \\
	$L_{(2)}, \beta=1.0$ & 0.063(0.001) & 0.073(0.0009) & 0.094(0.0008) & 0.177(0.005) \\
	\hline
\end{tabular}
	\end{center}
\caption{A table for test results evaluating influence of considering the volume fraction at loss function. 
The loss function $L_{(1)}$ is a default loss as in formula \ref{eq:loss_reg} and the loss function $L_{(2)}$ has an additional volume fraction penalty term multiplied by $\beta$ as in formula \ref{eq:loss_vfpen}. 
Every model was trained on the same training dataset. For every model average MSE and its variance is calculated. 
Test datasets with a known ($0.2$), intermediate ($0.3$) and larger ($0.5$) desired resulting volume fractions $V_{f}$ are used.}\label{tab:vf_full}
\end{table}
For the training set with intermediate volume fraction value, the model with volume fraction penalty and multiplicative constant of $0.2$ had an average MSE of $0.068$ and average volume error of $0.017$, comparing with $0.072$ and $0.02$ for a model without respectively.
Testing model for new, intermediate, values of the input volume fraction help producing accurate results.
\begin{figure}[H]
	\begin{subfigure}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=0.25\linewidth]{images/diff1_1_0200_28_6_03.png}
		\caption{Volume fraction value $0.3$}
		\label{fig:vf_example_1}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=0.25\linewidth]{images/diff1_1_0200_28_5_05}
		\caption{Volume fraction value $0.5$ }
		\label{fig:vf_example_2}
	\end{subfigure}
 \caption{Results for the network trained using volume fraction penalty loss for interpolating and extrapolating outcome volumes.}
	\label{fig:vf_example}
\end{figure}
Results extrapolation for input volume fraction larger than ones used for training, however, caused much worse results for models. 
For the best model, the average MSE in this case was $0.092$ and the average volume error was $0.64$.
This could be explained by convolutional nature of the model, with the fact that features found by the model are too subtle.  
The application of the post-processing allowed bringing the higher accuracy and better volume reconstruction. 
\begin{table}[h]
	\begin{center}
		\begin{tabular}{ |c|c|c|c|c| }
			\hline
			Loss vs. $E_{V}(Var)$ & $V_f=0.2$ & $V_f=0.3$& $V_f=0.5$ &  $V_f=0.3$, res. $28\times 28$\\ 
			\hline
			$\beta=0$ & 0.015(0.0001) & 0.020(0.0002) & 0.058(0.0004) & 0.054(0.001)  \\
			$\beta=0.01$ & 0.0017(0.0002) & 0.019(0.0002) & 0.007(0.0003) & 0.044(0.001)\\
			$\beta=0.2$ & 0.012(0.0001) & 0.017(0.0002)  & 0.064(0.0003)  & 0.046(0.001) \\
			$\beta=1.0$ & 0.011(0.001) & 0.031(0.0004) & 0.067(0.0003) & 0.07(0.002) \\
			\hline
		\end{tabular}
	\end{center}
	\caption{A table for test results evaluating influence of the volume fraction penalty term. Here we valuate the ability to reconstruct correct resulting volume.}\label{tab:vf_full_vf}
\end{table}
\medskip 

\subsubsection{Test III: Scaling problem resolution}
The next test had a goal of evaluation of the capability of the network to scale the results for different resolutions.
For this, we prepared training dataset with resolutions of previously unknown sizes.
For an intermediate size of the domain, the model shows a good accuracy of the interpolation.
For the larger resolutions, however, the ability to extrapolate was bad.
This could explained by the fact that features of the model describe only parts of much smaller models.
As an additional case, we increased the size of the domain, without changing the physics of the model
The result was also inaccurate with empty part of domain being populated with unphysical pixels.
This phenomenon was partially eliminated with post-processing.
\begin{figure}[h]
	\begin{subfigure}[b]{0.3\linewidth}
		\centering
		\includegraphics[width=0.25\linewidth]{images/diff1_1_0100_26_4_2802.png}
		\caption{Resolution $28 \times 28$}
		\label{fig:resols_1}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\linewidth}
	\centering
	\includegraphics[width=0.4\linewidth]{images/diff1_1_0200_26_4_6464.png}
	\caption{Resolution $64 \times 64$}
	\label{fig:resols_2}
\end{subfigure}
	\begin{subfigure}[b]{0.3\linewidth}
	\centering
	\includegraphics[width=0.5\linewidth]{images/diff1_1_0200_28_4_3264.png}
	\caption{Resolution $32 \times 64$}
	\label{fig:resols_3}
\end{subfigure}
	\caption{A results inferred by model for domain resolutions not used for training. Only for case of $28\times 28$ results are meaningful, however, not accurate. }
	\label{fig:resols}
\end{figure}
\medskip

\subsubsection{Test IV: Comparison with other architectures}
One of the tests goal was to compare the performance of different network architectures.
We compared our encoder-decoder residual network with similar architectures, used for medical image segmentation.
We considered \emph{V-Net}\cite{bibl:vnet} and \emph{U-Net}\cite{bibl:unet} architectures with similar encoder-decoder structure.
The former has $9,007,601$ number of parameters and the latter has $2,028,385$, comparing to $4,011,585$ for our model.
The U-Net network did not converge and was producing noise for any considered model configuration.
%The V-Net, having more parameters, in the end, was able to achieve comparable results, however, after much longer time spent training.
The V-Net achieved slightly better results for the evaluation set composed of cases with known resolution and volume.
However, for the data with an intermediate size of the domain and volume fraction value the results were significantly worse, as shown in table \ref{tab:vnet}.
Furthermore, the training of the V-net model took significantly more time.
\begin{table}[h]
	\begin{center}
		\begin{tabular}{ |c|c c|c c| }
			\hline
			Test Dataset & \multicolumn{2}{c|}{$32 \times 32, V_f=0.2$} & \multicolumn{2}{c|}{$28 \times 28, V_f=0.3$} \\
			\hline
			Model  & $MSE$ & $E_V$ & $MSE$ & $E_V$ \\ 
			\hline
			Our Architecture & 0.058(0.0008) & 0.012(0.0001) & 0.162(0.005) & 0.046(0.001) \\
			Our Architecture (w.o. SDF) & 0.048(0.0009) & 0.011(0.00006) & 0.172(0.004) & 0.072(0.001) \\
			V-net & 0.056(0.0008) & 0.013(0.0001) & 0.180(0.003) & 0.085(0.0005) \\
			\hline
		\end{tabular}
	\end{center}
	\caption{A table for test results for different models.}\label{tab:vnet}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.11\linewidth]{images/diff1_1_1200_23_1_vnet.png}
	\caption{A result produced by V-Net for a case with intemediate resolution and volume}
	\label{fig:vnet_pic}
\end{figure}
\medskip

\subsubsection{Test V: Influence of pre-processing}
The other test was done to evaluate the influence of applying SDF for inputs as pre-processing and justify such approach.
For that, we prepared two training datasets, one with initial bitmaps of the boundary conditions and load application points as inputs, and the second one with inputs after applying the pre-processing for every channel.
Models for both datasets were trained and then both were evaluated with two test sets, one is collection of TO cases for $32 \times 32$ domain size with $0.2$ of domain occupied by material, as well as for $28 \times 28 $ and $0.3$ respectively.
\medskip

The case without SDF show worse results for the data with new properties. 
However, this model shows better results for problem size and volume fraction used for training, as shown in table \ref{tab:vnet}.
The optimal model for a dataset without pre-processing also took more steps to train.
%The results show that with the pre-processing both time of training and final accuracy are much higher.
%The optimal model for the 
\medskip

\section{Results for 3D case}

In case of 3D domain, due to both smaller sample and more difficult perception of 3D data, we applied different visualization and evaluation methods.
In order to choose the best model average MSE across the test set was used.
The MSE variance was also used to estimate generality as well as accuracy heatmap.
\medskip 

However, as the main parameters, we took the influence of the inferred result on the conventional TO process, performed by IDeAs.
The first way is comparing the total time required need to perform TO. 
In case of reference value, it is the average time of a single IDeAs run for fixed domain size, and in case of our method, it is the average time for result inference by ANN system and average IDeAs time after the initialization with inferred results.
For these purposes, we had to introduce changes in IDeAs implementation, adding means to state initial VTK file describing the material layout and to initialize the data structures describing initial material densities with the value read from the VTK.
\medskip

As a different estimation of improvement, we measured how much the inferred result describe a bounding region of the ground-truth field after an increase of the volume with dilation.
We measure how many voxels of ground truth models averagely lie outside of the inferred bodies and the probability to be fully incorporated.
The estimation of dilation size applied to the network output required to cover the real result up to statistical significance is given.\todo{not made :(}

%This estimation was done in terms of the size of the probability of voxel not to be covered by bounding region and the decrease in the number of elements of the problem after we found the new domain. 

\subsubsection{Test I: Finding most accurate model for 3D data}

The first experiment was to find the best model after hyperparameter optimization.
We used the training dataset with sampling modes similar to the 2D case.
We pre-processed samples and the model had the loss function that included volume fraction penalty term.
After the process, similar to the 2D case, the best model was produced by the training dataset with forces at the edge and show accuracy of $0.14$ on the evaluation dataset, as seen from table \ref{tab:3d_valid}. 

\begin{table}[h]
	\begin{center}
		\begin{tabular}{ |c|c|c| }
			\hline
			 Sampling vs. Number of  iterations & $25k$ & $50k$ \\ 
			\hline
			Sparse grid sampling & 0.048(0.003) & 0.051(0.003) \\
			Sampling at the edges & 0.147(0.01) & 0.140(0.003) \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Validation results for models treating 3D data.}\label{tab:3d_valid}
\end{table}

Typical results look as shown in Figure \ref{fig:3dres_ex}.
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{images/24241600m1_4comp.png}
	\caption{An example output for of the network (red) compared to the reference model (blue). The model correctly connects the BC (corners of the $X=0$ wall) and the load application point, however the overall shape is very distinct from the ground truth.}
	\label{fig:3dres_ex}
\end{figure}

\subsubsection{Test II: TO acceleration with IDeAs}

After applying the elements layout produced by the following model as the input for the IDeAs, we achieved the average duration of optimization with the number of iterations equal $10$. 
This number is almost equal to the number of iterations performed by the optimizer starting with the default initialization, which is equal to $9$ iterations. 
%The current implementation of the optimizer uses a straightforward optimizer.
\medskip

In the case of initialization with an approximate solution, the objective function of the optimizer on the first function is already low. 
For the educated guess initializer it has value of averagely $7.9$, when the default initializer gives averagely $250$ in absolute units, and the found minimum in both cases is averagely $3.0$.
Starting in the vicinity of minimum is an ill-posed for current optimization rule, which means it is hard to utilize any educated initial guess approach to decrease the number of optimization iterations.
The objective function for many TO problems might have multiple local minima with domains of low gradient in their vicinity, which will require sophisticated optimization techniques for successful fine-tuning\todo{reference?}.

The convergence graph for TO with and without educated guess like as shown in figure \ref{}.
As it is seen, for the default implementation there is an initial phase during which the goal function magnitude decreases in two orders.
Then the phase of fine-tuning follows, which takes approximately two time longer.
For the case when educated guess is used, the convergence enter the fine-tuning phase after the first iteration. 
However, due the shape of goal function and a simple optimizer rule, this phase takes about $33%$ longer than for default phase.

In case when the optimization time is restricted, however, usage of the educated guess is beneficial.
We restricted the maximum amount of optimization iterations to $3$ and 

\subsubsection{Test III: Decreasing problem size}

%However, we with a probability of ... we manage to predict the new domain of the problem, close to the shape of the resulting model. 
The volume of the new domain was averagely ... percent smaller than the whole box and contained on only ... percent of empty voxels in the result.
That means that it possible to reduce the problem size by ..., than with proper implementation can significantly reduce optimization time. 

After averagely $4$ dilations we managed to make the network output cover the model for corresponding ground truth model with probability of $95\%$.


 


